Annie Gao
anniegao@usc.edu
My project is a NLP model that classifies news headlines as either sarcastic or not sarcastic.
I chose the News Headlines for Sarcasm Detection dataset. The only preprocessing step I applied was tokenization, which I did because I fine-tuned BERT, and BERT requires tokenization.
The model is implemented based on the L4 notebook, since both fine-tune BERT for binary NLP. My hyperparameters include batch size 32 and learning rate 2e-5, which ended up performing the best out of all the recommended values for batch size and learning rate, and I used 2 epochs since the validation accuracy did not significantly improve when using 3 or 4 epochs.
The metric I chose was MCC, since it evaluates performance across all classification outcomes (true positive, false negative, true negative, false positive). My model achieved a total MCC of .851, which indicates a strong agreement between predicted and actual labels.
The dataset, model architecture, training procedures, and chosen metrics do a good job of classifying news headlines as either sarcastic or not sarcastic. Implications of this project could be enhanced content moderation, since the model can identify sarcasm-based hate-speech that traditional systems might miss, or enhancing assistive technologies like chatbots or AI assistants to be able to deal with sarcastic user input. However, my methods are based on news headlines, which are shorter and more concise compared to other forms of text, which could be a limitation of extending this project. If I were to continue this project, I would experiment with different batch sizes and learning rates to further refine the model and add dropout layers to prevent overfitting.
